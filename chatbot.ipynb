{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "528c344624a74d448375ad1a12f38d13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_897558465f4c418d868009b28243d489",
              "IPY_MODEL_7489369f9f99456fb67a4cbdaac3b72d",
              "IPY_MODEL_339d4327d3d449afabda38f945757daf"
            ],
            "layout": "IPY_MODEL_2ea2b003de034ea99639acc8a2169f5b"
          }
        },
        "897558465f4c418d868009b28243d489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a02b83e8c8a497fbd776a639854b163",
            "placeholder": "​",
            "style": "IPY_MODEL_008f6a66f16a49c8b166ac7c38e94128",
            "value": "Map: 100%"
          }
        },
        "7489369f9f99456fb67a4cbdaac3b72d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31183d8efa9341968c785f91f75ca79e",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3143cb990bbd47cc81368286f07ed2cd",
            "value": 500
          }
        },
        "339d4327d3d449afabda38f945757daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e43309a7e0f430fb974ba4bd7417dc4",
            "placeholder": "​",
            "style": "IPY_MODEL_0fad4c70d4554e0890834ed8c2854ca8",
            "value": " 500/500 [00:02&lt;00:00, 242.46 examples/s]"
          }
        },
        "2ea2b003de034ea99639acc8a2169f5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a02b83e8c8a497fbd776a639854b163": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "008f6a66f16a49c8b166ac7c38e94128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31183d8efa9341968c785f91f75ca79e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3143cb990bbd47cc81368286f07ed2cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e43309a7e0f430fb974ba4bd7417dc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fad4c70d4554e0890834ed8c2854ca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQoB70midCYU",
        "outputId": "4fa760ed-3c4d-4dbc-ce97-19693a19f4b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Aug 26 14:18:31 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P0              27W /  70W |   4019MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "metadata": {
        "id": "dRRo-IDcU50a"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "l3V0KO4QViMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3590397c-38b7-456d-d65c-0a45bf1b30aa"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"squad\")\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2QZ0PBCU6oC",
        "outputId": "bbb71046-b459-4e2e-f767-e1db981ab4ca"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 87599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10570\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to make text bold\n",
        "s_bold = '\\033[1m'\n",
        "e_bold = '\\033[0;0m'\n",
        "\n",
        "print(s_bold + 'Train Data Sample.....' + e_bold)\n",
        "train_data = dataset[\"train\"]\n",
        "for data in train_data:\n",
        "    print(' ')\n",
        "    print(s_bold + 'ID -' + e_bold, data['id'])\n",
        "    print(s_bold +'TITLE - '+ e_bold, data['title'])\n",
        "    print(s_bold + 'CONTEXT - '+ e_bold,data['context'])\n",
        "    print(s_bold + 'ANSWERS - ' + e_bold,data['answers']['text'])\n",
        "    print(s_bold + 'ANSWERS START INDEX - ' + e_bold,data['answers']['answer_start'])\n",
        "    print(' ')\n",
        "    break\n",
        "\n",
        "print('---'*30)\n",
        "print(s_bold + 'Validation Data Sample.....' + e_bold)\n",
        "train_data = dataset[\"validation\"]\n",
        "for data in train_data:\n",
        "    print(' ')\n",
        "    print(s_bold + 'ID -' + e_bold, data['id'])\n",
        "    print(s_bold +'TITLE - '+ e_bold, data['title'])\n",
        "    print(s_bold + 'CONTEXT - '+ e_bold,data['context'])\n",
        "    print(s_bold + 'ANSWERS - ' + e_bold,data['answers']['text'])\n",
        "    print(s_bold + 'ANSWERS START INDEX - ' + e_bold,data['answers']['answer_start'])\n",
        "    print(' ')\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpAxfcecWOYZ",
        "outputId": "787809db-5cf8-4da1-a27b-2728d9fb314c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mTrain Data Sample.....\u001b[0;0m\n",
            " \n",
            "\u001b[1mID -\u001b[0;0m 5733be284776f41900661182\n",
            "\u001b[1mTITLE - \u001b[0;0m University_of_Notre_Dame\n",
            "\u001b[1mCONTEXT - \u001b[0;0m Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            "\u001b[1mANSWERS - \u001b[0;0m ['Saint Bernadette Soubirous']\n",
            "\u001b[1mANSWERS START INDEX - \u001b[0;0m [515]\n",
            " \n",
            "------------------------------------------------------------------------------------------\n",
            "\u001b[1mValidation Data Sample.....\u001b[0;0m\n",
            " \n",
            "\u001b[1mID -\u001b[0;0m 56be4db0acb8001400a502ec\n",
            "\u001b[1mTITLE - \u001b[0;0m Super_Bowl_50\n",
            "\u001b[1mCONTEXT - \u001b[0;0m Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "\u001b[1mANSWERS - \u001b[0;0m ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\n",
            "\u001b[1mANSWERS START INDEX - \u001b[0;0m [177, 177, 177]\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkMuEywdVAbm",
        "outputId": "dee9f00c-2423-451e-fa3b-d06649e17848"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "    num_rows: 0\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"validation\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igWCwv-3WiX-",
        "outputId": "a715103a-f750-451c-fb7f-3994a2ce7820"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "    num_rows: 10567\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets sample some dataset so that we can reduce training time.\n",
        "dataset[\"train\"] = dataset[\"train\"].select([i for i in range(8000)])\n",
        "dataset[\"validation\"] = dataset[\"validation\"].select([i for i in range(2000)])\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2pv45huWmkR",
        "outputId": "6629ad71-de8d-4ff4-90be-471f16a502b0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 8000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# model_checkpoint = \"bert-base-cased\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "trained_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
        "\n",
        "context = dataset[\"train\"][0][\"context\"]\n",
        "question = dataset[\"train\"][0][\"question\"]\n",
        "answer = dataset[\"train\"][0][\"answers\"][\"text\"]\n",
        "\n",
        "\n",
        "inputs = tokenizer(\n",
        "    question,\n",
        "    context,\n",
        "    max_length=160,\n",
        "    truncation=\"only_second\",  # only to truncate context\n",
        "    stride=70,  # no of overlapping tokens  between concecute context pieces\n",
        "    return_overflowing_tokens=True,  #to let tokenizer know we want overflow tokens\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
        "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")\n",
        "\n",
        "print('Question: ',question)\n",
        "print(' ')\n",
        "print('Context : ',context)\n",
        "print(' ')\n",
        "print('Answer: ', answer)\n",
        "print('--'*25)\n",
        "\n",
        "for i,ids in enumerate(inputs[\"input_ids\"]):\n",
        "    print('Context piece', i+1)\n",
        "    print(tokenizer.decode(ids[ids.index(102):]))\n",
        "    print(' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhjzfs-BWu2n",
        "outputId": "6a992a3d-4b59-4d40-b4e3-de7c6b1a7a0f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 4 examples gave 2 features.\n",
            "Here is where each comes from: [0, 0].\n",
            "Question:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            " \n",
            "Context :  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            " \n",
            "Answer:  ['Saint Bernadette Soubirous']\n",
            "--------------------------------------------------\n",
            "Context piece 1\n",
            "[SEP] architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues [SEP]\n",
            " \n",
            "Context piece 2\n",
            "[SEP] sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP]\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "del tokenizer\n",
        "trained_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
        "\n",
        "def train_data_preprocess(examples):\n",
        "\n",
        "    \"\"\"\n",
        "    generate start and end indexes of answer in context\n",
        "    \"\"\"\n",
        "\n",
        "    def find_context_start_end_index(sequence_ids):\n",
        "        \"\"\"\n",
        "        returns the token index in whih context starts and ends\n",
        "        \"\"\"\n",
        "        token_idx = 0\n",
        "        while sequence_ids[token_idx] != 1:  #means its special tokens or tokens of queston\n",
        "            token_idx += 1                   # loop only break when context starts in tokens\n",
        "        context_start_idx = token_idx\n",
        "\n",
        "        while sequence_ids[token_idx] == 1:\n",
        "            token_idx += 1\n",
        "        context_end_idx = token_idx - 1\n",
        "        return context_start_idx,context_end_idx\n",
        "\n",
        "\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    context = examples[\"context\"]\n",
        "    answers = examples[\"answers\"]\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        context,\n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,  #returns id of base context\n",
        "        return_offsets_mapping=True,  # returns (start_index,end_index) of each token\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "\n",
        "    for i,mapping_idx_pairs in enumerate(inputs['offset_mapping']):\n",
        "        context_idx = inputs['overflow_to_sample_mapping'][i]\n",
        "\n",
        "        # from main context\n",
        "        answer = answers[context_idx]\n",
        "        answer_start_char_idx = answer['answer_start'][0]\n",
        "        answer_end_char_idx = answer_start_char_idx + len(answer['text'][0])\n",
        "\n",
        "\n",
        "        # now we have to find it in sub contexts\n",
        "        tokens = inputs['input_ids'][i]\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # finding the context start and end indexes wrt sub context tokens\n",
        "        context_start_idx,context_end_idx = find_context_start_end_index(sequence_ids)\n",
        "\n",
        "        #if the answer is not fully inside context label it as (0,0)\n",
        "        # starting and end index of charecter of full context text\n",
        "        context_start_char_index = mapping_idx_pairs[context_start_idx][0]\n",
        "        context_end_char_index = mapping_idx_pairs[context_end_idx][1]\n",
        "\n",
        "\n",
        "        #If the answer is not fully inside the context, label is (0, 0)\n",
        "        if (context_start_char_index > answer_start_char_idx) or (\n",
        "            context_end_char_index < answer_end_char_idx):\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "\n",
        "        else:\n",
        "\n",
        "            # else its start and end token positions\n",
        "            # here idx indicates index of token\n",
        "            idx = context_start_idx\n",
        "            while idx <= context_end_idx and mapping_idx_pairs[idx][0] <= answer_start_char_idx:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "\n",
        "            idx = context_end_idx\n",
        "            while idx >= context_start_idx and mapping_idx_pairs[idx][1] > answer_end_char_idx:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n",
        "\n",
        "train_sample = dataset[\"train\"].select([i for i in range(200)])\n",
        "\n",
        "train_dataset = train_sample.map(\n",
        "    train_data_preprocess,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "\n",
        "len(dataset[\"train\"]),len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yShjyMWaXcUj",
        "outputId": "6cbf585d-8d2b-49e8-f79c-61a012654d8a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_context_and_answer(idx,mini_ds=dataset[\"train\"]):\n",
        "\n",
        "    print(idx)\n",
        "    print('----')\n",
        "    question = mini_ds[idx]['question']\n",
        "    context = mini_ds[idx]['context']\n",
        "    answer = mini_ds[idx]['answers']['text']\n",
        "    print('Theoretical values :')\n",
        "    print(' ')\n",
        "    print('Question: ')\n",
        "    print(question)\n",
        "    print(' ')\n",
        "    print('Context: ')\n",
        "    print(context)\n",
        "    print(' ')\n",
        "    print('Answer: ')\n",
        "    print(answer)\n",
        "    print(' ')\n",
        "    answer_start_char_idx = mini_ds[idx]['answers']['answer_start'][0]\n",
        "    answer_end_char_idx = answer_start_char_idx + len(mini_ds[idx]['answers']['text'][0])\n",
        "    print('Start and end index of text: ',answer_start_char_idx,answer_end_char_idx)\n",
        "    print('----'*20)\n",
        "    print('Values after tokenization:')\n",
        "\n",
        "\n",
        "    #answer\n",
        "    sep_tok_index = train_dataset[idx]['input_ids'].index(102) #get index for [SEP]\n",
        "    question_ = train_dataset[idx]['input_ids'][:sep_tok_index+1]\n",
        "    question_decoded = tokenizer.decode(question_)\n",
        "    context_ = train_dataset[idx]['input_ids'][sep_tok_index+1:]\n",
        "    context_decoded = tokenizer.decode(context_)\n",
        "    start_idx = train_dataset[idx]['start_positions']\n",
        "    end_idx = train_dataset[idx]['end_positions']\n",
        "    answer_toks = train_dataset[idx]['input_ids'][start_idx:end_idx]\n",
        "    answer_decoded = tokenizer.decode(answer_toks)\n",
        "    print(' ')\n",
        "    print('Question: ')\n",
        "    print(question_decoded)\n",
        "    print(' ')\n",
        "    print('Context: ')\n",
        "    print(context_decoded)\n",
        "    print(' ')\n",
        "    print('Answer: ')\n",
        "    print(answer_decoded)\n",
        "    print(' ')\n",
        "    print('Start pos and end pos of tokens: ',train_dataset[idx]['start_positions'],train_dataset[idx]['end_positions'])\n",
        "    print('____'*20)\n",
        "\n",
        "\n",
        "print_context_and_answer(0)\n",
        "print_context_and_answer(1)\n",
        "print_context_and_answer(2)\n",
        "print_context_and_answer(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQpUc4OlXxhi",
        "outputId": "7accb502-4c11-4e80-c9e6-42a441f3043b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "----\n",
            "Theoretical values :\n",
            " \n",
            "Question: \n",
            "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            " \n",
            "Context: \n",
            "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            " \n",
            "Answer: \n",
            "['Saint Bernadette Soubirous']\n",
            " \n",
            "Start and end index of text:  515 541\n",
            "--------------------------------------------------------------------------------\n",
            "Values after tokenization:\n",
            " \n",
            "Question: \n",
            "[CLS] to whom did the virgin mary allegedly appear in 1858 in lourdes france? [SEP]\n",
            " \n",
            "Context: \n",
            "architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            " \n",
            "Answer: \n",
            "saint bernadette soubirous\n",
            " \n",
            "Start pos and end pos of tokens:  130 138\n",
            "________________________________________________________________________________\n",
            "1\n",
            "----\n",
            "Theoretical values :\n",
            " \n",
            "Question: \n",
            "What is in front of the Notre Dame Main Building?\n",
            " \n",
            "Context: \n",
            "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            " \n",
            "Answer: \n",
            "['a copper statue of Christ']\n",
            " \n",
            "Start and end index of text:  188 213\n",
            "--------------------------------------------------------------------------------\n",
            "Values after tokenization:\n",
            " \n",
            "Question: \n",
            "[CLS] what is in front of the notre dame main building? [SEP]\n",
            " \n",
            "Context: \n",
            "architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            " \n",
            "Answer: \n",
            "a copper statue of christ\n",
            " \n",
            "Start pos and end pos of tokens:  52 57\n",
            "________________________________________________________________________________\n",
            "2\n",
            "----\n",
            "Theoretical values :\n",
            " \n",
            "Question: \n",
            "The Basilica of the Sacred heart at Notre Dame is beside to which structure?\n",
            " \n",
            "Context: \n",
            "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            " \n",
            "Answer: \n",
            "['the Main Building']\n",
            " \n",
            "Start and end index of text:  279 296\n",
            "--------------------------------------------------------------------------------\n",
            "Values after tokenization:\n",
            " \n",
            "Question: \n",
            "[CLS] the basilica of the sacred heart at notre dame is beside to which structure? [SEP]\n",
            " \n",
            "Context: \n",
            "architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            " \n",
            "Answer: \n",
            "the main building\n",
            " \n",
            "Start pos and end pos of tokens:  81 84\n",
            "________________________________________________________________________________\n",
            "3\n",
            "----\n",
            "Theoretical values :\n",
            " \n",
            "Question: \n",
            "What is the Grotto at Notre Dame?\n",
            " \n",
            "Context: \n",
            "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            " \n",
            "Answer: \n",
            "['a Marian place of prayer and reflection']\n",
            " \n",
            "Start and end index of text:  381 420\n",
            "--------------------------------------------------------------------------------\n",
            "Values after tokenization:\n",
            " \n",
            "Question: \n",
            "[CLS] what is the grotto at notre dame? [SEP]\n",
            " \n",
            "Context: \n",
            "architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            " \n",
            "Answer: \n",
            "a marian place of prayer and reflection\n",
            " \n",
            "Start pos and end pos of tokens:  95 102\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "def preprocess_validation_examples(examples):\n",
        "    \"\"\"\n",
        "    preprocessing validation data\n",
        "    \"\"\"\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    base_ids = []\n",
        "\n",
        "    for i in range(len(inputs[\"input_ids\"])):\n",
        "\n",
        "        # take the base id (ie in cases of overflow happens we get base id)\n",
        "        base_context_idx = sample_map[i]\n",
        "        base_ids.append(examples[\"id\"][base_context_idx])\n",
        "\n",
        "        # sequence id indicates the input. 0 for first input and 1 for second input\n",
        "        # and None for special tokens by default\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        offset = inputs[\"offset_mapping\"][i]\n",
        "        # for Question tokens provide offset_mapping as None\n",
        "        inputs[\"offset_mapping\"][i] = [\n",
        "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
        "        ]\n",
        "\n",
        "    inputs[\"base_id\"] = base_ids\n",
        "    return inputs\n",
        "\n",
        "\n",
        "# del tokenizer\n",
        "\n",
        "trained_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
        "\n",
        "data_val_sample = dataset[\"validation\"].select([i for i in range(100)])\n",
        "eval_set = data_val_sample.map(\n",
        "    preprocess_validation_examples,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"validation\"].column_names,\n",
        ")\n",
        "len(eval_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf69ZRz3X4k9",
        "outputId": "de4b677b-aac6-41e1-b700-2498ba9e7e8f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertForQuestionAnswering\n",
        "\n",
        "# del tokenizer\n",
        "# take a small sample\n",
        "\n",
        "eval_set_for_model = eval_set.remove_columns([\"base_id\", \"offset_mapping\"])\n",
        "eval_set_for_model.set_format(\"torch\")\n",
        "\n",
        "checkpoint =  \"distilbert-base-uncased\"\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
        "\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(checkpoint).to(\n",
        "    device\n",
        ")\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**batch)\n",
        "\n",
        "start_logits = outputs.start_logits.cpu().numpy()\n",
        "end_logits = outputs.end_logits.cpu().numpy()\n",
        "\n",
        "start_logits.shape,end_logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kko00HzOYBOR",
        "outputId": "1b71cbb4-5b4d-48ec-d579-3a69a11f935b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((100, 512), (100, 512))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9CP9-7nYGRl",
        "outputId": "bff1deb6-b56c-4dd0-e195-a318d994c139"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "import evaluate\n",
        "\n",
        "def predict_answers_and_evaluate(start_logits,end_logits,eval_set,examples):\n",
        "    \"\"\"\n",
        "    make predictions\n",
        "    Args:\n",
        "    start_logits : strat_position prediction logits\n",
        "    end_logits: end_position prediction logits\n",
        "    eval_set: processed val data\n",
        "    examples: unprocessed val data with context text\n",
        "    \"\"\"\n",
        "    # appending all id's corresponding to the base context id\n",
        "    example_to_features = collections.defaultdict(list)\n",
        "    for idx, feature in enumerate(eval_set):\n",
        "        example_to_features[feature[\"base_id\"]].append(idx)\n",
        "\n",
        "    n_best = 20\n",
        "    max_answer_length = 30\n",
        "    predicted_answers = []\n",
        "\n",
        "    for example in examples:\n",
        "        example_id = example[\"id\"]\n",
        "        context = example[\"context\"]\n",
        "        answers = []\n",
        "\n",
        "        # looping through each sub contexts corresponding to a context and finding\n",
        "        # answers\n",
        "        for feature_index in example_to_features[example_id]:\n",
        "            start_logit = start_logits[feature_index]\n",
        "            end_logit = end_logits[feature_index]\n",
        "            offsets = eval_set[\"offset_mapping\"][feature_index]\n",
        "\n",
        "            # sorting the predictions of all hidden states and taking best n_best prediction\n",
        "            # means taking the index of top 20 tokens\n",
        "            start_indexes = np.argsort(start_logit).tolist()[::-1][:n_best]\n",
        "            end_indexes = np.argsort(end_logit).tolist()[::-1][:n_best]\n",
        "\n",
        "\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "\n",
        "                    # Skip answers that are not fully in the context\n",
        "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
        "                        continue\n",
        "                    # Skip answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if (\n",
        "                        end_index < start_index\n",
        "                        or end_index - start_index + 1 > max_answer_length\n",
        "                       ):\n",
        "                        continue\n",
        "\n",
        "                    answers.append({\n",
        "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
        "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
        "                        })\n",
        "\n",
        "\n",
        "            # Select the answer with the best score\n",
        "        if len(answers) > 0:\n",
        "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
        "            predicted_answers.append(\n",
        "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
        "            )\n",
        "        else:\n",
        "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
        "\n",
        "    metric = evaluate.load(\"squad\")\n",
        "\n",
        "    theoretical_answers = [\n",
        "            {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples\n",
        "    ]\n",
        "\n",
        "    metric_ = metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
        "    return predicted_answers,metric_"
      ],
      "metadata": {
        "id": "QOLgWR-OYM1B"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_answers,metrics_ = predict_answers_and_evaluate(start_logits,end_logits,eval_set,data_val_sample)\n",
        "metrics_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVtP4NjGYVG_",
        "outputId": "1a7fc7dd-613d-4734-9b62-1694a4bad14a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exact_match': 2.0, 'f1': 5.878787878787878}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"squad\")\n",
        "\n",
        "#lets sample a small dataset\n",
        "dataset['train'] = dataset['train'].select([i for i in range(5000)])\n",
        "dataset['validation'] = dataset['validation'].select([i for i in range(500)])\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bevGw8YTYjeF",
        "outputId": "1b9cb7a1-9078-4e69-bcc4-7a52e524a223"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class DataQA(Dataset):\n",
        "    def __init__(self, dataset,mode=\"train\"):\n",
        "        self.mode = mode\n",
        "\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            # sampling\n",
        "            self.dataset = dataset[\"train\"]\n",
        "            self.data = self.dataset.map(train_data_preprocess,\n",
        "                                                      batched=True,\n",
        "                            remove_columns= dataset[\"train\"].column_names)\n",
        "\n",
        "        else:\n",
        "            self.dataset = dataset[\"validation\"]\n",
        "            self.data = self.dataset.map(preprocess_validation_examples,\n",
        "            batched=True,remove_columns = dataset[\"validation\"].column_names,\n",
        "               )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        out = {}\n",
        "        example = self.data[idx]\n",
        "        out['input_ids'] = torch.tensor(example['input_ids'])\n",
        "        out['attention_mask'] = torch.tensor(example['attention_mask'])\n",
        "\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "\n",
        "            out['start_positions'] = torch.unsqueeze(torch.tensor(example['start_positions']),dim=0)\n",
        "            out['end_positions'] = torch.unsqueeze(torch.tensor(example['end_positions']),dim=0)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "-FBo1aREYoQL"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint, model_max_length=1024)\n",
        "\n",
        "\n",
        "train_dataset = DataQA(dataset,mode=\"train\")\n",
        "val_dataset = DataQA(dataset,mode=\"validation\")\n",
        "\n",
        "\n",
        "\n",
        "for i,d in enumerate(train_dataset):\n",
        "    for k in d.keys():\n",
        "        print(k + ' : ', d[k].shape)\n",
        "    print('--'*40)\n",
        "\n",
        "    if i == 3:\n",
        "        break\n",
        "\n",
        "print('__'*50)\n",
        "\n",
        "for i,d in enumerate(val_dataset):\n",
        "    for k in d.keys():\n",
        "        print(k + ' : ', len(d[k]))\n",
        "    print('--'*40)\n",
        "\n",
        "    if i == 3:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639,
          "referenced_widgets": [
            "528c344624a74d448375ad1a12f38d13",
            "897558465f4c418d868009b28243d489",
            "7489369f9f99456fb67a4cbdaac3b72d",
            "339d4327d3d449afabda38f945757daf",
            "2ea2b003de034ea99639acc8a2169f5b",
            "7a02b83e8c8a497fbd776a639854b163",
            "008f6a66f16a49c8b166ac7c38e94128",
            "31183d8efa9341968c785f91f75ca79e",
            "3143cb990bbd47cc81368286f07ed2cd",
            "8e43309a7e0f430fb974ba4bd7417dc4",
            "0fad4c70d4554e0890834ed8c2854ca8"
          ]
        },
        "id": "eF3tG6itYtrO",
        "outputId": "aade6a4b-c33a-42eb-d166-5f5255e595eb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "528c344624a74d448375ad1a12f38d13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids :  torch.Size([512])\n",
            "attention_mask :  torch.Size([512])\n",
            "start_positions :  torch.Size([1])\n",
            "end_positions :  torch.Size([1])\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  torch.Size([512])\n",
            "attention_mask :  torch.Size([512])\n",
            "start_positions :  torch.Size([1])\n",
            "end_positions :  torch.Size([1])\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  torch.Size([512])\n",
            "attention_mask :  torch.Size([512])\n",
            "start_positions :  torch.Size([1])\n",
            "end_positions :  torch.Size([1])\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  torch.Size([512])\n",
            "attention_mask :  torch.Size([512])\n",
            "start_positions :  torch.Size([1])\n",
            "end_positions :  torch.Size([1])\n",
            "--------------------------------------------------------------------------------\n",
            "____________________________________________________________________________________________________\n",
            "input_ids :  512\n",
            "attention_mask :  512\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  512\n",
            "attention_mask :  512\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  512\n",
            "attention_mask :  512\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  512\n",
            "attention_mask :  512\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import default_data_collator\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    collate_fn=default_data_collator,\n",
        "    batch_size=2,\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    val_dataset, collate_fn=default_data_collator, batch_size=2\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for batch in train_dataloader:\n",
        "   print(batch['input_ids'].shape)\n",
        "   print(batch['attention_mask'].shape)\n",
        "   print(batch['start_positions'].shape)\n",
        "   print(batch['end_positions'].shape)\n",
        "   break\n",
        "\n",
        "print('---'*20)\n",
        "\n",
        "for batch in eval_dataloader:\n",
        "   print(batch['input_ids'].shape)\n",
        "   print(batch['attention_mask'].shape)\n",
        "   break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKXFbWgvYypw",
        "outputId": "8a63810a-3134-4930-96cf-259d6f6c4760"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512])\n",
            "torch.Size([2, 512])\n",
            "torch.Size([2, 1])\n",
            "torch.Size([2, 1])\n",
            "------------------------------------------------------------\n",
            "torch.Size([2, 512])\n",
            "torch.Size([2, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForQuestionAnswering\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Available device: {device}')\n",
        "\n",
        "checkpoint =  \"distilbert-base-uncased\"\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(checkpoint)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPs3EKssY4uG",
        "outputId": "8678ce78-ed3f-4222-c801-412b14e123c8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "import datetime\n",
        "import numpy as np\n",
        "import collections\n",
        "import evaluate\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "print(total_steps)\n",
        "\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE2PapxsZCDb",
        "outputId": "3a781883-82dc-48a1-d7f4-5b05ef767b7b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we need processed validation data to get offsets at the time of evaluation\n",
        "validation_processed_dataset = dataset[\"validation\"].map(preprocess_validation_examples,\n",
        "            batched=True,remove_columns = dataset[\"validation\"].column_names,\n",
        "               )"
      ],
      "metadata": {
        "id": "uB414rSoZID9"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random,time\n",
        "import numpy as np\n",
        "\n",
        "# to reproduce results\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "\n",
        "#storing all training and validation stats\n",
        "stats = []\n",
        "\n",
        "\n",
        "#to measure total training time\n",
        "total_train_time_start = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(' ')\n",
        "    print(f'=====Epoch {epoch + 1}=====')\n",
        "    print('Training....')\n",
        "\n",
        "    # ===============================\n",
        "    #    Train\n",
        "    # ===============================\n",
        "    # measure how long training epoch takes\n",
        "    t0 = time.time()\n",
        "\n",
        "    training_loss = 0\n",
        "    # loop through train data\n",
        "    model.train()\n",
        "    for step,batch in enumerate(train_dataloader):\n",
        "\n",
        "        # we will print train time in every 40 epochs\n",
        "        if step%40 == 0 and not step == 0:\n",
        "              elapsed_time = format_time(time.time() - t0)\n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed_time))\n",
        "\n",
        "\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "\n",
        "\n",
        "\n",
        "        #set gradients to zero\n",
        "        model.zero_grad()\n",
        "\n",
        "        result = model(input_ids = input_ids,\n",
        "                        attention_mask = attention_mask,\n",
        "                        start_positions = start_positions,\n",
        "                        end_positions = end_positions,\n",
        "                        return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "\n",
        "        #accumulate the loss over batches so that we can calculate avg loss at the end\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        #perform backward prorpogation\n",
        "        loss.backward()\n",
        "\n",
        "        # update the gradients\n",
        "        optimizer.step()\n",
        "\n",
        "    # calculate avg loss\n",
        "    avg_train_loss = training_loss/len(train_dataloader)\n",
        "\n",
        "    # calculates training time\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "\n",
        "    # ===============================\n",
        "    #    Validation\n",
        "    # ===============================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    start_logits,end_logits = [],[]\n",
        "    for step,batch in enumerate(eval_dataloader):\n",
        "\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():\n",
        "             result = model(input_ids = input_ids,\n",
        "                        attention_mask = attention_mask,return_dict=True)\n",
        "\n",
        "\n",
        "\n",
        "        start_logits.append(result.start_logits.cpu().numpy())\n",
        "        end_logits.append(result.end_logits.cpu().numpy())\n",
        "\n",
        "\n",
        "    start_logits = np.concatenate(start_logits)\n",
        "    end_logits = np.concatenate(end_logits)\n",
        "    # start_logits = start_logits[: len(val_dataset)]\n",
        "    # end_logits = end_logits[: len(val_dataset)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # calculating metrics\n",
        "    answers,metrics_ = predict_answers_and_evaluate(start_logits,end_logits,validation_processed_dataset,dataset[\"validation\"])\n",
        "    print(f'Exact match: {metrics_[\"exact_match\"]}, F1 score: {metrics_[\"f1\"]}')\n",
        "\n",
        "\n",
        "    print('')\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_train_time_start)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0ytyo8zZKWH",
        "outputId": "e06a2d27-3046-43be-dd23-60bf35a37151"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "=====Epoch 1=====\n",
            "Training....\n",
            "  Batch    40  of  2,505.    Elapsed: 0:00:06.\n",
            "  Batch    80  of  2,505.    Elapsed: 0:00:11.\n",
            "  Batch   120  of  2,505.    Elapsed: 0:00:15.\n",
            "  Batch   160  of  2,505.    Elapsed: 0:00:20.\n",
            "  Batch   200  of  2,505.    Elapsed: 0:00:24.\n",
            "  Batch   240  of  2,505.    Elapsed: 0:00:29.\n",
            "  Batch   280  of  2,505.    Elapsed: 0:00:33.\n",
            "  Batch   320  of  2,505.    Elapsed: 0:00:38.\n",
            "  Batch   360  of  2,505.    Elapsed: 0:00:42.\n",
            "  Batch   400  of  2,505.    Elapsed: 0:00:47.\n",
            "  Batch   440  of  2,505.    Elapsed: 0:00:51.\n",
            "  Batch   480  of  2,505.    Elapsed: 0:00:55.\n",
            "  Batch   520  of  2,505.    Elapsed: 0:01:00.\n",
            "  Batch   560  of  2,505.    Elapsed: 0:01:04.\n",
            "  Batch   600  of  2,505.    Elapsed: 0:01:09.\n",
            "  Batch   640  of  2,505.    Elapsed: 0:01:14.\n",
            "  Batch   680  of  2,505.    Elapsed: 0:01:18.\n",
            "  Batch   720  of  2,505.    Elapsed: 0:01:23.\n",
            "  Batch   760  of  2,505.    Elapsed: 0:01:27.\n",
            "  Batch   800  of  2,505.    Elapsed: 0:01:32.\n",
            "  Batch   840  of  2,505.    Elapsed: 0:01:37.\n",
            "  Batch   880  of  2,505.    Elapsed: 0:01:41.\n",
            "  Batch   920  of  2,505.    Elapsed: 0:01:46.\n",
            "  Batch   960  of  2,505.    Elapsed: 0:01:50.\n",
            "  Batch 1,000  of  2,505.    Elapsed: 0:01:55.\n",
            "  Batch 1,040  of  2,505.    Elapsed: 0:01:59.\n",
            "  Batch 1,080  of  2,505.    Elapsed: 0:02:04.\n",
            "  Batch 1,120  of  2,505.    Elapsed: 0:02:09.\n",
            "  Batch 1,160  of  2,505.    Elapsed: 0:02:13.\n",
            "  Batch 1,200  of  2,505.    Elapsed: 0:02:18.\n",
            "  Batch 1,240  of  2,505.    Elapsed: 0:02:22.\n",
            "  Batch 1,280  of  2,505.    Elapsed: 0:02:27.\n",
            "  Batch 1,320  of  2,505.    Elapsed: 0:02:31.\n",
            "  Batch 1,360  of  2,505.    Elapsed: 0:02:36.\n",
            "  Batch 1,400  of  2,505.    Elapsed: 0:02:40.\n",
            "  Batch 1,440  of  2,505.    Elapsed: 0:02:45.\n",
            "  Batch 1,480  of  2,505.    Elapsed: 0:02:49.\n",
            "  Batch 1,520  of  2,505.    Elapsed: 0:02:54.\n",
            "  Batch 1,560  of  2,505.    Elapsed: 0:02:59.\n",
            "  Batch 1,600  of  2,505.    Elapsed: 0:03:03.\n",
            "  Batch 1,640  of  2,505.    Elapsed: 0:03:08.\n",
            "  Batch 1,680  of  2,505.    Elapsed: 0:03:12.\n",
            "  Batch 1,720  of  2,505.    Elapsed: 0:03:17.\n",
            "  Batch 1,760  of  2,505.    Elapsed: 0:03:21.\n",
            "  Batch 1,800  of  2,505.    Elapsed: 0:03:26.\n",
            "  Batch 1,840  of  2,505.    Elapsed: 0:03:30.\n",
            "  Batch 1,880  of  2,505.    Elapsed: 0:03:35.\n",
            "  Batch 1,920  of  2,505.    Elapsed: 0:03:39.\n",
            "  Batch 1,960  of  2,505.    Elapsed: 0:03:44.\n",
            "  Batch 2,000  of  2,505.    Elapsed: 0:03:48.\n",
            "  Batch 2,040  of  2,505.    Elapsed: 0:03:53.\n",
            "  Batch 2,080  of  2,505.    Elapsed: 0:03:58.\n",
            "  Batch 2,120  of  2,505.    Elapsed: 0:04:02.\n",
            "  Batch 2,160  of  2,505.    Elapsed: 0:04:07.\n",
            "  Batch 2,200  of  2,505.    Elapsed: 0:04:11.\n",
            "  Batch 2,240  of  2,505.    Elapsed: 0:04:16.\n",
            "  Batch 2,280  of  2,505.    Elapsed: 0:04:20.\n",
            "  Batch 2,320  of  2,505.    Elapsed: 0:04:25.\n",
            "  Batch 2,360  of  2,505.    Elapsed: 0:04:29.\n",
            "  Batch 2,400  of  2,505.    Elapsed: 0:04:34.\n",
            "  Batch 2,440  of  2,505.    Elapsed: 0:04:38.\n",
            "  Batch 2,480  of  2,505.    Elapsed: 0:04:43.\n",
            "\n",
            "  Average training loss: 2.33\n",
            "  Training epoch took: 0:04:46\n",
            "\n",
            "Running Validation...\n",
            "Exact match: 23.4, F1 score: 59.22810318680287\n",
            "\n",
            "  Validation took: 0:04:27\n",
            " \n",
            "=====Epoch 2=====\n",
            "Training....\n",
            "  Batch    40  of  2,505.    Elapsed: 0:00:05.\n",
            "  Batch    80  of  2,505.    Elapsed: 0:00:09.\n",
            "  Batch   120  of  2,505.    Elapsed: 0:00:14.\n",
            "  Batch   160  of  2,505.    Elapsed: 0:00:18.\n",
            "  Batch   200  of  2,505.    Elapsed: 0:00:23.\n",
            "  Batch   240  of  2,505.    Elapsed: 0:00:28.\n",
            "  Batch   280  of  2,505.    Elapsed: 0:00:32.\n",
            "  Batch   320  of  2,505.    Elapsed: 0:00:37.\n",
            "  Batch   360  of  2,505.    Elapsed: 0:00:42.\n",
            "  Batch   400  of  2,505.    Elapsed: 0:00:46.\n",
            "  Batch   440  of  2,505.    Elapsed: 0:00:51.\n",
            "  Batch   480  of  2,505.    Elapsed: 0:00:55.\n",
            "  Batch   520  of  2,505.    Elapsed: 0:01:00.\n",
            "  Batch   560  of  2,505.    Elapsed: 0:01:04.\n",
            "  Batch   600  of  2,505.    Elapsed: 0:01:09.\n",
            "  Batch   640  of  2,505.    Elapsed: 0:01:13.\n",
            "  Batch   680  of  2,505.    Elapsed: 0:01:18.\n",
            "  Batch   720  of  2,505.    Elapsed: 0:01:22.\n",
            "  Batch   760  of  2,505.    Elapsed: 0:01:27.\n",
            "  Batch   800  of  2,505.    Elapsed: 0:01:31.\n",
            "  Batch   840  of  2,505.    Elapsed: 0:01:36.\n",
            "  Batch   880  of  2,505.    Elapsed: 0:01:40.\n",
            "  Batch   920  of  2,505.    Elapsed: 0:01:45.\n",
            "  Batch   960  of  2,505.    Elapsed: 0:01:49.\n",
            "  Batch 1,000  of  2,505.    Elapsed: 0:01:54.\n",
            "  Batch 1,040  of  2,505.    Elapsed: 0:01:58.\n",
            "  Batch 1,080  of  2,505.    Elapsed: 0:02:03.\n",
            "  Batch 1,120  of  2,505.    Elapsed: 0:02:08.\n",
            "  Batch 1,160  of  2,505.    Elapsed: 0:02:12.\n",
            "  Batch 1,200  of  2,505.    Elapsed: 0:02:17.\n",
            "  Batch 1,240  of  2,505.    Elapsed: 0:02:21.\n",
            "  Batch 1,280  of  2,505.    Elapsed: 0:02:26.\n",
            "  Batch 1,320  of  2,505.    Elapsed: 0:02:30.\n",
            "  Batch 1,360  of  2,505.    Elapsed: 0:02:35.\n",
            "  Batch 1,400  of  2,505.    Elapsed: 0:02:39.\n",
            "  Batch 1,440  of  2,505.    Elapsed: 0:02:44.\n",
            "  Batch 1,480  of  2,505.    Elapsed: 0:02:48.\n",
            "  Batch 1,520  of  2,505.    Elapsed: 0:02:53.\n",
            "  Batch 1,560  of  2,505.    Elapsed: 0:02:57.\n",
            "  Batch 1,600  of  2,505.    Elapsed: 0:03:02.\n",
            "  Batch 1,640  of  2,505.    Elapsed: 0:03:06.\n",
            "  Batch 1,680  of  2,505.    Elapsed: 0:03:11.\n",
            "  Batch 1,720  of  2,505.    Elapsed: 0:03:16.\n",
            "  Batch 1,760  of  2,505.    Elapsed: 0:03:20.\n",
            "  Batch 1,800  of  2,505.    Elapsed: 0:03:25.\n",
            "  Batch 1,840  of  2,505.    Elapsed: 0:03:29.\n",
            "  Batch 1,880  of  2,505.    Elapsed: 0:03:34.\n",
            "  Batch 1,920  of  2,505.    Elapsed: 0:03:38.\n",
            "  Batch 1,960  of  2,505.    Elapsed: 0:03:43.\n",
            "  Batch 2,000  of  2,505.    Elapsed: 0:03:47.\n",
            "  Batch 2,040  of  2,505.    Elapsed: 0:03:52.\n",
            "  Batch 2,080  of  2,505.    Elapsed: 0:03:56.\n",
            "  Batch 2,120  of  2,505.    Elapsed: 0:04:01.\n",
            "  Batch 2,160  of  2,505.    Elapsed: 0:04:05.\n",
            "  Batch 2,200  of  2,505.    Elapsed: 0:04:10.\n",
            "  Batch 2,240  of  2,505.    Elapsed: 0:04:14.\n",
            "  Batch 2,280  of  2,505.    Elapsed: 0:04:19.\n",
            "  Batch 2,320  of  2,505.    Elapsed: 0:04:23.\n",
            "  Batch 2,360  of  2,505.    Elapsed: 0:04:28.\n",
            "  Batch 2,400  of  2,505.    Elapsed: 0:04:32.\n",
            "  Batch 2,440  of  2,505.    Elapsed: 0:04:37.\n",
            "  Batch 2,480  of  2,505.    Elapsed: 0:04:42.\n",
            "\n",
            "  Average training loss: 1.01\n",
            "  Training epoch took: 0:04:44\n",
            "\n",
            "Running Validation...\n",
            "Exact match: 25.4, F1 score: 63.46824086073314\n",
            "\n",
            "  Validation took: 0:04:14\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:18:11 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./fine-tuned-bert-qa\")\n",
        "tokenizer.save_pretrained(\"./fine-tuned-bert-qa\")"
      ],
      "metadata": {
        "id": "bZw3LwAywqii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3124fe0d-8de7-44b4-8289-25b24457cbb3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./fine-tuned-bert-qa/tokenizer_config.json',\n",
              " './fine-tuned-bert-qa/special_tokens_map.json',\n",
              " './fine-tuned-bert-qa/vocab.txt',\n",
              " './fine-tuned-bert-qa/added_tokens.json',\n",
              " './fine-tuned-bert-qa/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"./fine-tuned-bert-qa\", tokenizer=\"./fine-tuned-bert-qa\")\n",
        "\n",
        "context = \"\"\"Machine learning is an exciting branch of Artificial Intelligence, and it’s all around us. Machine learning brings out the power of data in new ways, such as Facebook suggesting articles in your feed. This amazing technology helps computer systems learn and improve from experience by developing computer programs that can automatically access data and perform tasks via predictions and detections.\n",
        "\n",
        "As you input more data into a machine, this helps the algorithms teach the computer, thus improving the delivered results. When you ask Alexa to play your favorite music station on Amazon Echo, she will go to the station you played most often. You can further improve and refine your listening experience by telling Alexa to skip songs, adjust the volume, and many more possible commands. Machine Learning and the rapid advance of Artificial Intelligence makes this all possible.\"\"\"\n",
        "question = \"Which is branch of Artificial Intelligence?\"\n",
        "\n",
        "result = qa_pipeline(question=question, context=context)\n",
        "print(result[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_3yDkRo5ST_",
        "outputId": "4eaa53e7-9204-4083-84a3-2f5a7b0aa5ec"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8b-WWHhQJS2_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}